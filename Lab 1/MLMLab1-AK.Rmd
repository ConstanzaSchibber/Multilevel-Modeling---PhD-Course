---
title: 'MLM: Lab 1'
author: "Constanza F. Schibber"
date: "1/20/2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

## Installing & Loading Packages 

```{r Install Packages}
# to recode variables
#install.packages('car') 
# to read data from other software
#install.packages('foreign')
#install.packages('MASS')
# this is from the Gelman and Hill book
#install.packages('arm') 
#install.packages('apsrtable')
```

```{r load packages}
library(arm) 
library(car) # function `recode` allows you to recode a variable
library(apsrtable)
library(foreign)
```

# Chapter 2: Concepts and methods from basic probability and statistics

## Section 2.3

```{r CI}
# CI for continuous data
y <- c(35,34,38,35,37)
n <- length(y)
estimate <- mean(y)
se <- sd(y)/sqrt(n)
int.50 <- estimate + qt(c(.25,.75),n-1)*se
int.95 <- estimate + qt(c(.025,.975),n-1)*se
int.95
# CI for proportions
y <- 700
n <- 1000
estimate <- y/n
se <- sqrt (estimate*(1-estimate)/n)
int.95 <- estimate + qnorm(c(.025,.975))*se
int.95
```

Figure 2.3, which displays the proportion of American adults supporting the death penalty (among those with an opinion on the question), from a series of Gallup polls. 

```{r Plot Figure 2.3}
par (mar=c(5,5,4,2)+.1)
# Read data
polls <- matrix (scan("http://www.stat.columbia.edu/~gelman/arm/examples/death.polls/polls.dat"), ncol=5, byrow=TRUE)
# Variables
support <- polls[,3]/(polls[,3]+polls[,4])
year <-  polls[,1] + (polls[,2]-6)/12
# Start figure
plot (year, support*100, 
      xlab="Year", 
      ylim=c(min(100*support)-1, max(100*support)+1),
      ylab="Percentage support for \n the death penalty", 
      cex=1.1, 
      cex.main=1.2,
      cex.axis=1.1, 
      cex.lab=1.1, 
      pch=20)
# Add SE
for(i in 1:nrow(polls)){
    lines(rep(year[i],2),
    100*(support[i]+c(-1,1)*sqrt(support[i]*(1-support[i])/1000)))
}
```

For an example of a formal comparison, consider a change in the estimated support for the death penalty from $80\% \pm 1.4\%$ to $74\% \pm 1.3\%$. The estimated difference is:

```{r difference}
difference<-80-74
difference
se.difference<- sqrt(1.4^2+1.3^2)
se.difference
```

# Chapter 3: Linear Regression. The basics.

## 3.1 One predictor

```{r iq 3.1}
kidiq <- read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/kidiq.dta")
colnames(kidiq)
head(kidiq)

## Fit 0, 
fit.0 <- lm(kid_score ~ mom_hs, data=kidiq)
display(fit.0)

## Plot Figure 3.1
kidscore.jitter <- jitter(kidiq$kid_score)

jitter.binary <- function(a, jitt=.05){
   ifelse (a==0, runif (length(a), 0, jitt), runif (length(a), 1-jitt, 1))
}

jitter.mom_hs <- jitter.binary(kidiq$mom_hs)

plot(jitter.mom_hs,
     kidscore.jitter, 
     xlab="Mother completed high school", 
     ylab="Child test score",
     pch=20, 
     xaxt="n", yaxt="n")
axis (1, seq(0,1))
axis (2, c(20,60,100,140))
abline (fit.0)
```

```{r x is mom_iq}
fit.1 <- lm(kid_score ~ mom_iq, data=kidiq)

## Plot Figure 3.2
plot(kidiq$mom_iq,
     kidiq$kid_score, 
     xlab="Mother IQ score", 
     ylab="Child test score",
     pch=20, xaxt="n", yaxt="n")
axis (1, c(80,100,120,140))
axis (2, c(20,60,100,140))
abline (fit.1)
```

## 3.2 Multiple predictors

```{r 2 predictors}
fit.2 <- lm (kid_score ~ mom_hs + mom_iq, data=kidiq)

plot(kidiq$mom_iq,
     kidiq$kid_score, 
     xlab="Mother IQ score", 
     ylab="Child test score",
     pch=20, xaxt="n", yaxt="n", type="n")
curve (coef(fit.2)[1] + coef(fit.2)[2] + coef(fit.2)[3]*x, add=TRUE, col="gray")
curve (coef(fit.2)[1] + coef(fit.2)[3]*x, add=TRUE)
points (kidiq$mom_iq[kidiq$mom_hs==0], kidiq$kid_score[kidiq$mom_hs==0], pch=19)
points (kidiq$mom_iq[kidiq$mom_hs==1], kidiq$kid_score[kidiq$mom_hs==1], col="gray", pch=19)
axis (1, c(80,100,120,140))
axis (2, c(20,60,100,140))
```

## 3.3 Interactions

```{r lm interaction}
fit <- lm (kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, data=kidiq)
display (fit)

#par(mfrow=c(1,2))
## Figure 3.4 (a)
plot(kidiq$mom_iq,
     kidiq$kid_score, 
     xlab="Mother IQ score", 
     ylab="Child test score",
     pch=20, xaxt="n", yaxt="n", type="n")
curve (coef(fit)[1] + coef(fit)[2] + (coef(fit)[3] + coef(fit)[4])*x, add=TRUE, col="gray")
curve (coef(fit)[1] + coef(fit)[3]*x, add=TRUE)
points (kidiq$mom_iq[kidiq$mom_hs==0], kidiq$kid_score[kidiq$mom_hs==0], pch=20)
points (kidiq$mom_iq[kidiq$mom_hs==1], kidiq$kid_score[kidiq$mom_hs==1], col="gray", pch=20)
axis (1, c(80,100,120,140))
axis (2, c(20,60,100,140))

## Figure 3.4 (b)
plot(kidiq$mom_iq,
     kidiq$kid_score, 
     xlab="Mother IQ score", 
     ylab="Child test score",
     pch=20, type="n", xlim=c(0,150), ylim=c(0,150))
curve(coef(fit)[1] + coef(fit)[2] + (coef(fit)[3] + coef(fit)[4])*x, add=TRUE, col="gray")
curve (coef(fit)[1] + coef(fit)[3]*x, add=TRUE)
points (kidiq$mom_iq[kidiq$mom_hs==0], kidiq$kid_score[kidiq$mom_hs==0], pch=20)
points (kidiq$mom_iq[kidiq$mom_hs==1], kidiq$kid_score[kidiq$mom_hs==1], col="gray", pch=20)

# 2nd figure is the same plot but with horizontal axis extended to zero to reveal the intercepts of the lines.
```

## 3.5 Graphical displays of data and fitted model

```{r understading code for figures}
## Regression line as a function of one input variable
fit.2 <- lm (kid_score ~ mom_iq, data = kidiq)

plot (kidiq$mom_iq, 
      kidiq$kid_score, 
      xlab="Mother IQ score", 
      ylab="Child test score")
curve (coef(fit.2)[1] + coef(fit.2)[2]*x, add=TRUE)

 # alternately
curve (cbind(1,x) %*% coef(fit.2), add=TRUE)

### Two fitted regression lines

## model with no interaction
fit.3 <- lm(kid_score ~ mom_hs + mom_iq, data=kidiq)

# Figure
plot (kidiq$mom_iq, 
      kidiq$kid_score, 
      xlab="Mother IQ score", 
      ylab="Child test score", pch=20)
curve(cbind (1, 1, x) %*% coef(fit.3), add=TRUE, col="black")
curve(cbind (1, 0, x) %*% coef(fit.3), add=TRUE, col="gray")

  # alternative sequence of commands
plot(kidiq$mom_iq, 
     kidiq$kid_score, 
     xlab="Mother IQ score", 
     ylab="Child test score",
     type="n")
points(kidiq$mom_iq[kidiq$mom_hs==1],
       kidiq$kid_score[kidiq$mom_hs==1], 
       pch=20, 
       col="black")
points (kidiq$mom_iq[kidiq$mom_hs==0], 
        kidiq$kid_score[kidiq$mom_hs==0], 
        pch=20, 
        col="gray")
curve(cbind (1, 1, x) %*% coef(fit.3), add=TRUE, col="black")
curve(cbind (1, 0, x) %*% coef(fit.3), add=TRUE, col="gray")

## model with interaction
fit.4 <- lm(kid_score ~ mom_hs + 
              mom_iq + 
              mom_hs:mom_iq, data=kidiq)
plot(kidiq$mom_iq, 
     kidiq$kid_score, 
     xlab="Mother IQ score", 
     ylab="Child test score", pch=20)
curve(cbind (1, 1, x, 1*x) %*% coef(fit.4), add=TRUE, col="black")
curve(cbind (1, 0, x, 0*x) %*% coef(fit.4), add=TRUE, col="gray")
```

```{r Figure 3.10}
### Displaying uncertainty in the fitted regression (Figure 3.10)
fit.2 <- lm (kid_score ~ mom_iq, data= kidiq)
display(fit.2)

fit.2.sim <- sim(fit.2)
plot(kidiq$mom_iq, kidiq$kid_score, 
      xlab="Mother IQ score", 
      ylab="Child test score", 
      pch=20)
for (i in 1:10){
  curve(coef(fit.2.sim)[i,1] + coef(fit.2.sim)[i,2]*x, add=TRUE,col="gray")
}
curve (coef(fit.2)[1] + coef(fit.2)[2]*x, add=TRUE, col="red")
```

## 3.6 Assumptions and diagnostics

```{r residuals}
fit.2 <- lm (kid_score ~ mom_iq, data=kidiq)
resid <- fit.2$residuals
sd.resid <- sd(resid)

 # Figure 3.12
plot(kidiq$mom_iq, resid, 
      xlab="Mother IQ score", 
      ylab="Residuals", 
      pch=20)
abline (sd.resid,0,lty=2)
abline(0,0)
abline (-sd.resid,0,lty=2)
```

## 3.7 Prediction and validation

```{r validation}
## Model fit and prediction
fit.3 <- lm (kid_score ~ mom_hs + mom_iq, data=kidiq)
x.new <- data.frame(mom_hs=1, mom_iq=100)
predict(fit.3, x.new, interval="prediction", level=0.95)

## Figure 3.13
#The version of the dataset to do this is not available. It included a new variable. 
```

## Exercise 3.4 -- Practice!

The child.iq folder contains a subset of the children and mother data discussed earlier in the chapter. You have access to children’s test scores at age 3, mother’s education, and the mother’s age at the time she gave birth for a sample of 400 children. 

(a) Fit a regression of child test scores on mother's age, display the data and fitted model, check assumptions, and interpret the slope coefficient. When do you recommend mothers should give birth? What are you assuming in making these recommendations?

(b) Repeat this for a regression that further includes mother's education, inter- preting both slope coefficients in this model. Have your conclusions about the timing of birth changed?

(c) Now create an indicator variable reflecting whether the mother has completed high school or not. Consider interactions between the high school completion and mother's age in family. Also, create a plot that shows the separate regression lines for each high school completion status group.

(d) Finally, fit a regression of child test scores on mother’s age and education level for the first 200 children and use this model to predict test scores for the next 200. Graphically display comparisons of the predicted and actual scores for the final 200 children.

```{r exercise 3.4}
iq.data<-read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/child.iq/child.iq.dta")

# (a) 

model<-lm(ppvt~momage, data=iq.data)

plot(iq.data$momage, iq.data$ppvt,
     xlab="Mom's Age", 
     ylab="Child's Test Score")
curve(coef(model)[1]+ coef(model)[2]*x, add=TRUE )

#Diagonostics
par(mfrow=c(1,2))
plot(model$fit, model$res, xlab="Fitted", ylab="Residuals", pch=20)
abline(h=0) 
plot(model$fit, abs(model$res), xlab="Fitted", ylab="|Residuals|", pch=20)

#PLOT

par(mfrow=c(1,2)) 
plot(ppvt ~ momage, 
     data=iq.data, 
     xlim=c(16, 32), 
     ylim=c(20, 160), 
     xlab="Mother’s age at Child’s Birth", 
     ylab="Child’s Test Score", 
     type="n") 
colors <- ifelse(iq.data$educ_cat==1, "black", 
                 ifelse(iq.data$educ_cat==2, "gray30", 
                        ifelse(iq.data$educ_cat==3, "gray60", "gray90"))) 
points(ppvt ~ momage, data=iq.data, col=colors, pch=20)
curve(cbind(1,x) %*% coef(model), add=TRUE, lwd=2, col="blue")
legend(x=25, y=165, c("No High School", "High School", "Some college", "College "), 
       col=c( "black", "gray30", "gray60", "gray90"), 
       pch=20, 
       cex=0.6)

# b)
model2<-lm(ppvt~momage+educ_cat, data=iq.data)

#Diagonostics
par(mfrow=c(1,2))
plot(model2$fit, model2$res, xlab="Fitted", ylab="Residuals", pch=20)
abline(h=0) 
plot(model2$fit, abs(model2$res), xlab="Fitted", ylab="|Residuals|", pch=20)

#DATA AND MODEL
beta.hat<-coef(model2)
beta.sim<-sim(model2)

# 2 plots
par(mfrow=c(1,2))
plot(iq.data$educ_cat, iq.data$ppvt, 
     xlab="Mother's Educations", 
     ylab="Child's Test Scores", 
     pch=20)
for(i in 1:10){
	curve(cbind(1, mean(iq.data$momage),x)%*%beta.hat,col="gray",lwd=5, add=TRUE)
	}
	curve(cbind(1, mean(iq.data$momage),x)%*% coef(beta.sim)[i,], col="black",add=TRUE)
	
plot(iq.data$momage, iq.data$ppvt, 
     xlab="Mother's age", 
     ylab="Child's Test Score",
     pch=20)
	for(i in 1:10){
	curve(cbind(1,x,mean(iq.data$educ_cat))%*%coef(beta.sim)[i,],col="gray",lwd=5, add=TRUE)
	}
	curve(cbind(1,x,mean(iq.data$educ_cat))%*% beta.hat, col="black",add=TRUE)
	
# c

# d
first200<-iq.data[1:200,]
last200<-iq.data[201:400,]

```

# Chapter 4: Linear regression: before and after fitting the model

## 4.1 Linear transformations

```{r earnings}
heights <- read.dta ("http://www.stat.columbia.edu/~gelman/arm/examples/earnings/heights.dta")

# create a new gender variable 
heights$male <- 2 - heights$sex

# (for simplicity) remove cases with missing data
heights<-subset(heights, height!="NA")
dim(heights)

#  earn =0 is also missing data 
heights<-subset(heights, earn>0)
dim(heights)

## Model fit
lm.earn <- lm(earn ~ height, data=heights)
display (lm.earn)
sim.earn <- sim(lm.earn)
beta.hat <- coef(lm.earn)

height.jitter.add <- runif(nrow(heights), -.2, .2)
## Figure 4.1 (left)
par (mar=c(6,6,4,2)+.1, mfrow=c(1,2))
plot (heights$height + height.jitter.add, 
      heights$earn, 
      xlab="height", 
      ylab="earnings", 
      pch=20, 
      mgp=c(4,2,0), 
      yaxt="n", 
      col="gray10",
     main="Fitted linear model")
axis (2, c(0,100000,200000), c("0","100000","200000"), mgp=c(4,1.1,0))
for (i in 1:20){
  curve (coef(sim.earn)[i,1] + coef(sim.earn)[i,2]*x, 
         lwd=.5,
         col="gray", 
         add=TRUE)}
curve (beta.hat[1] + beta.hat[2]*x, add=TRUE, col="red")

## Figure 4.1 (right) 
par (mar=c(6,6,4,2)+.1)
plot (heights$height + height.jitter.add, 
      heights$earn, 
      xlab="height", 
      ylab="earnings", 
      pch=20, 
      mgp=c(4,2,0), 
      yaxt="n", 
      col="gray10",
      main="Fitted linear model",
      xlim=c(0,80),
      ylim=c(-200000,200000))
axis (2, c(-100000,0,100000), c("-100000","0","100000"), mgp=c(4,1.1,0))
for (i in 1:20){
  curve (coef(sim.earn)[i,1] + coef(sim.earn)[i,2]*x, 
         lwd=.5, 
         col="gray", 
         add=TRUE)}
curve (beta.hat[1] + beta.hat[2]*x, add=TRUE, col="red")
```

## 4.2 Centering and standardizing, especially for models with interactions

```{r kidiq}
fit.4 <- lm (kid_score ~ mom_hs + mom_iq + mom_hs:mom_iq, 
             data=kidiq)
display(fit.4)

 # centering by subtracting the mean
c_mom_hs <- kidiq$mom_hs - mean(kidiq$mom_hs)
c_mom_iq <- kidiq$mom_iq - mean(kidiq$mom_iq)

fit.5 <- lm (kid_score ~ c_mom_hs + c_mom_iq + 
               c_mom_hs:c_mom_iq, kidiq)
display(fit.5)

 # using a conventional centering point
c2_mom_hs <- kidiq$mom_hs - 0.5
c2_mom_iq <- kidiq$mom_iq - 100

fit.6 <- lm (kid_score ~ c2_mom_hs + c2_mom_iq +
               c2_mom_hs:c2_mom_iq, data=kidiq)
display(fit.6)

 # centering by subtracting the mean & dividing by 2 sd
z_mom_hs <- (kidiq$mom_hs - mean(kidiq$mom_hs))/(2*sd(kidiq$mom_hs))
z_mom_iq <- (kidiq$mom_iq - mean(kidiq$mom_iq))/(2*sd(kidiq$mom_iq))

fit.7 <- lm (kid_score ~ z_mom_hs + z_mom_iq + 
               z_mom_hs:z_mom_iq, data= kidiq)
display(fit.7)
```

## 4.4 Logarithmic transformations

```{r log}
log.earn <- log(heights$earn)
earn.logmodel.1 <- lm(log.earn ~ height, data = heights)
display(earn.logmodel.1)
 
# Figure 4.3
sim.logmodel.1 <- sim(earn.logmodel.1)
beta.hat <- coef(earn.logmodel.1)

# plot
par (mar=c(6,6,4,2)+.1, mfrow=c(1,2))
plot (heights$height + runif(n,-.2,.2), 
      heights$log.earn, 
      xlab="height", 
      ylab="log (earnings)", 
      pch=20, yaxt="n", 
      mgp=c(4,2,0), 
      col="gray10",
      main="Log regression, plotted on log scale")
axis (2, seq(6,12,2), mgp=c(4,1.1,0))
for (i in 1:20){
  curve(coef(sim.logmodel.1)[i,1] + coef(sim.logmodel.1)[i,2]*x, 
        lwd=.5, 
        col="gray", 
        add=TRUE)
}
curve (beta.hat[1] + beta.hat[2]*x, add=TRUE, col="red")

# plot
plot (heights$height + runif(n,-.2,.2), 
      heights$earn, 
      xlab="height", 
      ylab="earnings", 
      pch=20, 
      yaxt="n", 
      mgp=c(4,2,0), 
      col="gray10",
      main="Log regression, plotted on original scale")
axis (2, c(0,100000,200000), c("0","100000","200000"), mgp=c(4,1.1,0))
for (i in 1:20){
  curve (exp(coef(sim.logmodel.1)[i,1] + coef(sim.logmodel.1)[i,2]*x), 
         lwd=.5, 
         col="gray", 
         add=TRUE)
}
curve(exp(beta.hat[1] + beta.hat[2]*x), add=TRUE, col="red")

## Log-base-10 transformation

log10.earn <- log10(heights$earn)
earn.log10model <- lm(log10.earn ~ height, data=heights)
display(earn.log10model)

## Log scale regression model

earn.logmodel.2 <- lm(log.earn ~ height + male, data=heights)
display(earn.logmodel.2)

## Including interactions

earn.logmodel.3 <- lm(log.earn ~ height + male + height:male, data=heights)
display(earn.logmodel.3)

## Linear transformations

z.height <- (heights$height - mean(heights$height))/sd(heights$height)
earn.logmodel.4 <- lm(log.earn ~ z.height + male + z.height:male, data= heights)
display(earn.logmodel.4)

## Log-log model

log.height <- log(heights$height)
earn.logmodel.5 <- lm(log.earn ~ log.height + male, heights)
display(earn.logmodel.5)
```

## Exercise 4.4 -- Practice!

# 4.4

Logarithmic transformations: the folder pollution contains mortality rates and various environmental factors from 60 U.S. metropolitan areas (see McDonald and Schwing, 1973). For this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transforma- tions in regression.

(a) Create a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.

(b) Find an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.

(c) Interpret the slope coefficient from the model you chose in (b). 

(d) Now fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when helpful. Plot the fitted regression model and interpret the coefficients. 

(e) Cross-validate: fit the model you chose above to the first half of the data and then predict for the second half. (You used all the data to construct the model in (d), so this is not really cross-validation)

```{r exercise 4.4}
library(foreign)

# We use foreign to read data in different formats, like SPSS - read.spss - or STATA files -read.dta
#help(read.dta)
# Reading the data
pollution<-read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/pollution/pollution.dta")

# Information about the dataset
colnames(pollution) # names of columns
head(pollution) # shows first rows
summary(pollution) # summary statistics
dim(pollution) # dimension of the dataset rows x columns
nrow(pollution) # number of rows
ncol(pollution) # number of columns

# Summary statistics about a variable
mean(pollution$mort) 
median(pollution$mort)
sd(pollution$mort)
var(pollution$mort)
max(pollution$mort)
min(pollution$mort)
range(pollution$mort)

# Plotting a variable
plot(density(pollution$mort))
plot(density(pollution$mort), col='red', lwd=3, main='Mortality Rate', xlab='')
abline(v=mean(pollution$mort))
abline(v=median(pollution$mort), lty=2)

hist(pollution$mort)
hist(pollution$mort,  breaks=30, xlim=c(750,1120), col='gray', xlab='Mortality Rate', main='Histogram')
hist(pollution$mort, breaks=30, xlim=c(750,1120), col='gray', xlab='Mortality Rate', main='Histogram', freq=FALSE)
lines(density(pollution$mort), lwd=3, col='red')

# Every figure
pairs(pollution)

# Subset of the data
summary(subset(pollution, mort>=900))
dim(subset(pollution, mort>=900))

pollution.subset<-subset(pollution, mort>=900)
dim(pollution.subset)

# Plotting 2 variables
plot(pollution$nox, pollution$mort, ylab="mortality rate", xlab="level of nitric oxides", pch=3)

# Fit a linear model y = mort, x=nox

## FILL IN 
#m<- lm(y~x, data=FILL IN NAME) 
#summary(m)
#display(m)

# Residual plot from the regression
#plot(residuals(m)~pollution$nox, xlab="level of nitric oxides",ylab="residuals", pch=3) 
#abline(h=0, col="gray70", lty=2)

# Fitted regression model 
plot(pollution$nox, pollution$mort, ylab="mortality rate", xlab="level of nitric oxides", pch=3)
#abline(m, ldw=3, col='blue')

# Y v. Predicted Y (Y Hat) 
#prediction <-predict(m, pollution)
#plot(prediction, pollution$mort, ylab="mortality rate", xlab="level of nitric oxides", pch=3)
#plot(prediction, pollution$nox, ylab="mortality rate", xlab="level of nitric oxides", pch=3)

# Variable transformation

# log() , exp() , Which one would you choose? (a) Choose one and remake the figure (b) Rewrite th label for the x axis to include the transformation 
# use par(mfrow=c(1,2)) to plot the 2 figures side by side 

# Fit a linear model using the transformed variable nox and mort as the outcome variable. Remake the residual plot. 

# 

# Crossvalidation
#(e)
#dim(pollution)
#testset<-pollution[1:30,]
#predset<-pollution[31:60,]

```

## 4.5 Other transformations

```{r factor}
fit <- lm (kid_score ~ as.factor(mom_work), kidiq)
display(fit)
```

# Chapter 5: Logistic Regression

Full code http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/arsenic_chap5.R

Background on page 87

## 5.4 Building a logistic regression model: Wells in Bangladesh

```{r wells data}
wells<-read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", header=TRUE)
head(wells)

# histogram of distances 
# Figure 5.8, page 88
#postscript ("c:/books/multilevel/arsenic.distances.bnew.ps", height=3, width=4, horizontal=TRUE)
hist (wells$dist, breaks=seq(0,10+max(wells$dist[!is.na(wells$dist)]),10), freq=TRUE, xlab="Distance (in meters) to nearest safe well", ylab="", main="", mgp=c(2,.5,0))
#dev.off ()
```

### Logistic regression with just one predictor

```{r one predictor}
# model 1
fit.1 <- glm (switch ~ dist, 
              family=binomial(link="logit"), 
              data= wells)
display(fit.1)

# Redefine distance in 100-meter units and fit the model again
wells$dist100 <- wells$dist/100

# model 1 again
fit.2 <- glm (switch ~ dist100, family=binomial(link="logit"), wells)
display(fit.2)

# Graphing the fitted model
jitter.binary <- function(a, jitt=.05){
  a + (1-2*a)*runif(length(a),0,jitt)
}

# Figure 5.9 page 89
#postscript ("c:/books/multilevel/arsenic.logitfit.1new.a.ps", height=3.5, width=4, horizontal=TRUE)
plot(c(0,max(wells$dist, na.rm=TRUE)*1.02), c(0,1), xlab="Distance (in meters) to nearest safe well", ylab="Pr (switching)", type="n", xaxs="i", yaxs="i", mgp=c(2,.5,0))
curve (invlogit(coef(fit.1)[1]+coef(fit.1)[2]*x), lwd=1, add=TRUE)
points (wells$dist, jitter.binary(wells$switch), pch=20, cex=.1)
#dev.off ()
```

#### Interpreting the logistic regression coefficients

```{r probabilities}
# When dist100=0
invlogit(coef(fit.2)[1])

# When dist100= mean using invlogit
invlogit(coef(fit.2)[1]+coef(fit.2)[2]*mean(wells$dist100))

# When dist100=mean using equation
linear.pred<-coef(fit.2)[1]+coef(fit.2)[2]*mean(wells$dist100)
exp(linear.pred)/(1+exp(linear.pred))

# What is the infinitesimal change in probability for the mean dist100 ---> derivative of equation above

coef(fit.2)[2]*exp(linear.pred)/(1+exp(linear.pred))^2
```

## Adding a second input variable and including interactions 

```{r two predictors}
# model with 2 predictors
fit.3 <- glm (switch ~ dist100 + arsenic, family=binomial(link="logit"), wells)
display (fit.3)

# including an interaction
fit.4 <- glm (switch ~ dist100 + arsenic + dist100:arsenic,
  family=binomial(link="logit"), wells)

# centering the input variables
wells$c.dist100 <- wells$dist100 - mean (wells$dist100)
wells$c.arsenic <- wells$arsenic - mean (wells$arsenic)

fit.5 <- glm (switch ~ c.dist100 + c.arsenic + c.dist100:c.arsenic,
  family=binomial(link="logit"), wells)

# Graphing the fitted model with two predictors
# Figure 5.11
par(mfrow=c(1,2))
## Panel 1
plot(c(0,max(wells$dist,na.rm=TRUE)*1.02), c(0,1), 
     xlab="Distance (in meters) to nearest safe well", 
     ylab="Pr (switching)", 
     type="n", 
     xaxs="i", 
     yaxs="i", 
     mgp=c(2,.5,0))

points (wells$dist, jitter.binary(wells$switch), pch=20, cex=.1)

curve (invlogit(coef(fit.4)[1]+coef(fit.4)[2]*x/100+coef(fit.4)[3]*.50+coef(fit.4)[4]*(x/100)*.50), 
       lwd=.5, 
       add=TRUE)

curve (invlogit(coef(fit.4)[1]+coef(fit.4)[2]*x/100+coef(fit.4)[3]*1.00+coef(fit.4)[4]*(x/100)*1.00), 
       lwd=.5, 
       add=TRUE)

text (50, .29, "if As = 0.5", adj=0, cex=.8)
text (75, .50, "if As = 1.0", adj=0, cex=.8)

## Panel 2
plot(c(0,max(wells$arsenic,na.rm=TRUE)*1.02), c(0,1), 
     xlab="Arsenic concentration in well water", 
     ylab="Pr (switching)", 
     type="n", 
     xaxs="i", 
     yaxs="i", 
     mgp=c(2,.5,0))

points (wells$arsenic, jitter.binary(wells$switch), pch=20, cex=.1)

curve (invlogit(coef(fit.4)[1]+coef(fit.4)[2]*0+coef(fit.4)[3]*x+coef(fit.4)[4]*0*x), from=0.5, lwd=.5, add=TRUE)

curve (invlogit(coef(fit.4)[1]+coef(fit.4)[2]*0.5+coef(fit.4)[3]*x+coef(fit.4)[4]*0.5*x), from=0.5, lwd=.5, add=TRUE)

text (.50, .78, "if dist = 0", adj=0, cex=.8)
text (2.00, .6, "if dist = 50", adj=0, cex=.8)

#
```

## Error rate

```{r error rate}
# center variables
wells$c.dist100 <- wells$dist100 - mean (wells$dist100)
wells$c.arsenic <- wells$arsenic - mean (wells$arsenic)
wells$c.educ4 <- wells$educ/4 - mean(wells$educ/4)


# fit 8
fit.8 <- glm (switch ~ c.dist100 + c.arsenic + c.educ4 + c.dist100:c.arsenic +
  c.dist100:c.educ4 + c.arsenic:c.educ4, family=binomial(link="logit"), wells)
display (fit.8)

# fitted values
pred.8 <- fit.8$fitted.values

# error rate
error.rate <- mean(round(abs(wells$switch-pred.8)))
error.rate.null <- mean(round(abs(wells$switch-mean(pred.8))))

## Equivalent to computing the error rates as...
predicted<-pred.8
y<-wells$switch
error.rate <- mean((predicted>0.5 & y==0) | (predicted<0.5 & y==1))
```

## Poisson - Police Stops (page 111)

The police stops example is overdispersed. Below is the code from the Gelman and Hill book.  

```{r}
library(arm)
# Read from the web
# The authors added some noise to the data to preserve anonimity so the results are a bit different from those presented by the book
# Bad shaped data so you have to do the following

X <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/police/frisk_with_noise.dat",skip=6,header=TRUE)

names(X)[3] <- "arrests"

X <- aggregate(cbind(stops, arrests) ~ precinct + eth, data=X, sum)

 # Model 2 - ethnicity indicator

fit.2 <- glm (stops ~ factor(eth), family=poisson,  data=X, offset=log(arrests))
summary(fit.2)

yhat <- predict(fit.2, type="response")
n<-225
k<-79
z <- (X$stops-yhat)/sqrt(yhat)
cat ("overdispersion ratio is ", sum(z^2)/(n-k), "\n")
cat ("p-value of overdispersion test is ", pchisq (sum(z^2), n-k), "\n")
```

 

